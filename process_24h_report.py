import asyncio
import os
import sys
import logging
import re
import json
from datetime import datetime, timedelta

# Ensure we can import from src
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from dotenv import load_dotenv
load_dotenv(override=True)

from src.config import config
from src.processors.summarizer import AISummarizer
from src.adapters.telegram_adapter_v2 import TelegramMultiAccountAdapter
from src.models import UnifiedMessage, Platform

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def estimate_token_count(text):
    """ç²—ç•¥ä¼°è®¡æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆè‹±æ–‡å•è¯æ•° + ä¸­æ–‡å­—ç¬¦æ•° * 2ï¼‰"""
    # ç®€å•ä¼°ç®—ï¼šè‹±æ–‡å•è¯æ•° + ä¸­æ–‡å­—ç¬¦æ•° * 2
    english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    # å…¶ä»–å­—ç¬¦ï¼ˆæ ‡ç‚¹ã€æ•°å­—ç­‰ï¼‰æŒ‰0.5å€è®¡ç®—
    other_chars = len(text) - english_words - chinese_chars
    return english_words + chinese_chars * 2 + int(other_chars * 0.5)

def chunk_messages_by_tokens(message_list, max_tokens_per_chunk=100000):
    """
    å°†æ¶ˆæ¯åˆ—è¡¨æŒ‰tokenæ•°åˆ†å—ï¼Œç¡®ä¿æ¯å—ä¸è¶…è¿‡é™åˆ¶
    è¿”å›åˆ†å—åˆ—è¡¨ï¼Œæ¯å—åŒ…å«æ¶ˆæ¯å’Œèµ·å§‹ID
    """
    if not message_list:
        return []
    
    chunks = []
    current_chunk = []
    current_tokens = 0
    current_start_id = 0
    
    for idx, msg in enumerate(message_list):
        # ä¼˜åŒ–æ¶ˆæ¯æ ¼å¼ï¼šåªä¿ç•™IDï¼Œå»æ‰ç”¨æˆ·å
        message_text = f"[ID:{idx}] {msg.content}"
        message_tokens = estimate_token_count(message_text)
        
        # å¦‚æœå½“å‰å—ä¸ºç©ºæˆ–æ·»åŠ è¿™æ¡æ¶ˆæ¯ä¸ä¼šè¶…è¿‡é™åˆ¶ï¼Œå°±æ·»åŠ åˆ°å½“å‰å—
        if not current_chunk or current_tokens + message_tokens <= max_tokens_per_chunk:
            current_chunk.append((idx, msg))
            current_tokens += message_tokens
        else:
            # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
            chunks.append({
                'start_id': current_start_id,
                'messages': current_chunk.copy(),
                'estimated_tokens': current_tokens
            })
            current_chunk = [(idx, msg)]
            current_tokens = message_tokens
            current_start_id = idx
    
    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk:
        chunks.append({
            'start_id': current_start_id,
            'messages': current_chunk,
            'estimated_tokens': current_tokens
        })
    
    logger.info(f"æ¶ˆæ¯åˆ†å—å®Œæˆï¼šå…± {len(message_list)} æ¡æ¶ˆæ¯ï¼Œåˆ†æˆ {len(chunks)} ä¸ªå—")
    for i, chunk in enumerate(chunks):
        logger.info(f"  å— {i+1}: {len(chunk['messages'])} æ¡æ¶ˆæ¯ï¼Œä¼°è®¡ {chunk['estimated_tokens']} tokens")
    
    return chunks

async def generate_chunk_summary(summarizer, chunk_data, chunk_index, total_chunks, start_time, end_time):
    """ç”Ÿæˆå•ä¸ªåˆ†å—çš„æ‘˜è¦"""
    # è¯»å– setting_AI.md
    try:
        with open("setting_AI.md", "r", encoding="utf-8") as f:
            setting_ai_content = f.read()
    except Exception as e:
        logger.error(f"è¯»å– setting_AI.md å¤±è´¥: {e}")
        setting_ai_content = "æ— æ³•è¯»å– setting_AI.mdï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚"

    # æ ¼å¼åŒ–æ—¶é—´èŒƒå›´
    time_range_str = f"{start_time.strftime('%m%d %H:%M')} - {end_time.strftime('%m%d %H:%M')}"

    # å‡†å¤‡åˆ†å—æ¶ˆæ¯æ–‡æœ¬
    messages_with_ids = []
    for original_id, msg in chunk_data['messages']:
        # ä½¿ç”¨ç›¸å¯¹IDï¼Œä»0å¼€å§‹
        relative_id = original_id - chunk_data['start_id']
        messages_with_ids.append(f"[ID:{relative_id}] {msg.content}")
    
    messages_text = "\n".join(messages_with_ids)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒºå—é“¾æŠ•ç ”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä»å¤šä¸ª Telegram ç¾¤ç»„é‡‡é›†åˆ°çš„ç¢ç‰‡åŒ–ä¿¡æ¯ï¼Œæ•´ç†å‡ºè¿™éƒ¨åˆ†ä¿¡æ¯çš„æ‘˜è¦ã€‚

    è¿™æ˜¯ç¬¬ {chunk_index + 1}/{total_chunks} ä¸ªåˆ†å—ã€‚

    è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è®¾å®šï¼ˆsetting_AI.mdï¼‰ï¼š
    {setting_ai_content}

    å½“å‰ç®€æŠ¥çš„æ—¶é—´èŒƒå›´æ˜¯ï¼š{time_range_str}

    é‡‡é›†åˆ°çš„åŸå§‹ä¿¡æ¯å¦‚ä¸‹ï¼ˆæ¯æ¡æ¶ˆæ¯éƒ½æœ‰IDæ ‡è®°ï¼‰ï¼š
    {messages_text}

    è¯·è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
    {{
      "summary": "è¿™éƒ¨åˆ†ä¿¡æ¯çš„æ‘˜è¦å†…å®¹ï¼Œé‡ç‚¹å…³æ³¨ï¼š1) ä¸»è¦è®¨è®ºä¸»é¢˜ 2) é‡è¦è¶‹åŠ¿ 3) é£é™©æç¤º 4) æŠ•èµ„æœºä¼š",
      "basic_question_ids": [0, 1, 2, ...]  // åŸºç¡€æ“ä½œé—®é¢˜çš„IDåˆ—è¡¨ï¼ˆç›¸å¯¹IDï¼‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºæ•°ç»„[]
    }}
    """
    
    try:
        # ä½¿ç”¨æ–°çš„summarizeræ¥å£
        result = await summarizer.generate_json_response(
            prompt=prompt,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒºå—é“¾æŠ•ç ”åŠ©æ‰‹ï¼Œä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„æ•´ç†é€»è¾‘ç”Ÿæˆåˆ†å—æ‘˜è¦ï¼Œå¹¶è¿”å›JSONæ ¼å¼çš„ç»“æœã€‚",
            temperature=0.3
        )
        
        # å°†ç›¸å¯¹IDè½¬æ¢å›åŸå§‹ID
        if result.get("basic_question_ids"):
            original_ids = []
            for relative_id in result["basic_question_ids"]:
                original_id = chunk_data['start_id'] + relative_id
                original_ids.append(original_id)
            result["basic_question_ids"] = original_ids
        
        return result
    except Exception as e:
        logger.error(f"åˆ†å— {chunk_index + 1} AI ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
        return {"summary": f"åˆ†å— {chunk_index + 1} AI æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}", "basic_question_ids": []}

async def generate_global_summary(summarizer, aggregated_text, message_list, start_time, end_time):
    """è°ƒç”¨ AI ç”Ÿæˆå…¨å±€æ‘˜è¦ï¼Œä½¿ç”¨åˆ†å—å¤„ç†ç­–ç•¥"""
    # è¯»å– setting_AI.md
    try:
        with open("setting_AI.md", "r", encoding="utf-8") as f:
            setting_ai_content = f.read()
    except Exception as e:
        logger.error(f"è¯»å– setting_AI.md å¤±è´¥: {e}")
        setting_ai_content = "æ— æ³•è¯»å– setting_AI.mdï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚"

    # æ ¼å¼åŒ–æ—¶é—´èŒƒå›´
    time_range_str = f"{start_time.strftime('%m%d %H:%M')} - {end_time.strftime('%m%d %H:%M')}"

    if not message_list:
        logger.warning("æ²¡æœ‰å¯ç”¨çš„æ¶ˆæ¯è¿›è¡Œæ‘˜è¦ç”Ÿæˆ")
        return {"summary": f"ğŸ“Š {time_range_str}\n\nâš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯ç”Ÿæˆç®€æŠ¥", "basic_question_ids": []}

    logger.info(f"å¼€å§‹å¤„ç† {len(message_list)} æ¡æ¶ˆæ¯çš„æ‘˜è¦ç”Ÿæˆ")
    
    # 1. å°†æ¶ˆæ¯åˆ†å—
    chunks = chunk_messages_by_tokens(message_list, max_tokens_per_chunk=100000)
    
    if not chunks:
        logger.warning("æ¶ˆæ¯åˆ†å—å¤±è´¥")
        return {"summary": f"ğŸ“Š {time_range_str}\n\nâš ï¸ æ¶ˆæ¯å¤„ç†å¤±è´¥", "basic_question_ids": []}
    
    # 2. å¹¶è¡Œç”Ÿæˆåˆ†å—æ‘˜è¦
    chunk_summaries = []
    all_basic_question_ids = []
    
    for chunk_index, chunk_data in enumerate(chunks):
        logger.info(f"å¤„ç†åˆ†å— {chunk_index + 1}/{len(chunks)}")
        chunk_result = await generate_chunk_summary(
            summarizer, chunk_data, chunk_index, len(chunks), start_time, end_time
        )
        chunk_summaries.append(chunk_result)
        
        # æ”¶é›†åŸºç¡€æ“ä½œé—®é¢˜ID
        if chunk_result.get("basic_question_ids"):
            all_basic_question_ids.extend(chunk_result["basic_question_ids"])
    
    # 3. èšåˆæ‰€æœ‰åˆ†å—æ‘˜è¦
    if len(chunks) == 1:
        # å¦‚æœåªæœ‰ä¸€ä¸ªåˆ†å—ï¼Œç›´æ¥ä½¿ç”¨å…¶æ‘˜è¦
        final_summary = chunk_summaries[0]["summary"]
    else:
        # å¦‚æœæœ‰å¤šä¸ªåˆ†å—ï¼Œéœ€è¦èšåˆ
        final_summary = await aggregate_chunk_summaries(
            summarizer, chunk_summaries, start_time, end_time, setting_ai_content
        )
    
    # 4. ç¡®ä¿æ‘˜è¦æ ¼å¼æ­£ç¡®
    if not final_summary.startswith("ğŸ“Š"):
        final_summary = f"ğŸ“Š {time_range_str}\n\n{final_summary}"
    
    return {
        "summary": final_summary,
        "basic_question_ids": all_basic_question_ids
    }

async def aggregate_chunk_summaries(summarizer, chunk_summaries, start_time, end_time, setting_ai_content):
    """èšåˆå¤šä¸ªåˆ†å—æ‘˜è¦ä¸ºå…¨å±€æ‘˜è¦"""
    # æ ¼å¼åŒ–æ—¶é—´èŒƒå›´
    time_range_str = f"{start_time.strftime('%m%d %H:%M')} - {end_time.strftime('%m%d %H:%M')}"
    
    # å‡†å¤‡æ‰€æœ‰åˆ†å—æ‘˜è¦
    chunk_summary_texts = []
    for i, chunk_result in enumerate(chunk_summaries):
        chunk_summary_texts.append(f"=== åˆ†å— {i+1} æ‘˜è¦ ===\n{chunk_result['summary']}")
    
    all_chunk_summaries = "\n\n".join(chunk_summary_texts)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒºå—é“¾æŠ•ç ”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¤šä¸ªåˆ†å—çš„æ‘˜è¦ï¼Œæ•´ç†å‡ºä¸€ä»½å®Œæ•´çš„æ·±åº¦ç®€æŠ¥ã€‚

    è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹è®¾å®šï¼ˆsetting_AI.mdï¼‰ï¼š
    {setting_ai_content}

    å½“å‰ç®€æŠ¥çš„æ—¶é—´èŒƒå›´æ˜¯ï¼š{time_range_str}
    è¯·ç¡®ä¿ç®€æŠ¥å¼€å¤´ä¸¥æ ¼æŒ‰ç…§è®¾å®šä¸­çš„æ ¼å¼ï¼šğŸ“Š {time_range_str}

    ä»¥ä¸‹æ˜¯ {len(chunk_summaries)} ä¸ªåˆ†å—çš„æ‘˜è¦ï¼š
    {all_chunk_summaries}

    è¯·ç”Ÿæˆä¸€ä»½å®Œæ•´çš„å…¨å±€ç®€æŠ¥ï¼Œè¦æ±‚ï¼š
    1. æ•´åˆæ‰€æœ‰åˆ†å—çš„å…³é”®ä¿¡æ¯
    2. è¯†åˆ«æ•´ä½“è¶‹åŠ¿å’Œæ¨¡å¼
    3. çªå‡ºæœ€é‡è¦çš„æŠ•èµ„æœºä¼šå’Œé£é™©
    4. ä¿æŒsetting_AI.mdä¸­è¦æ±‚çš„æ ¼å¼å’Œç»“æ„
    5. é¿å…é‡å¤ä¿¡æ¯ï¼Œè¿›è¡Œå»é‡å’Œæ•´åˆ

    è¯·ç›´æ¥è¿”å›å®Œæ•´çš„ç®€æŠ¥å†…å®¹ï¼ˆä¸éœ€è¦JSONæ ¼å¼ï¼‰ã€‚
    """
    
    try:
        # ä½¿ç”¨æ–°çš„summarizeræ¥å£
        final_summary = await summarizer.generate_summary_with_prompt(
            prompt=prompt,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒºå—é“¾æŠ•ç ”åŠ©æ‰‹ï¼Œæ“…é•¿æ•´åˆå¤šä¸ªåˆ†å—æ‘˜è¦ï¼Œç”Ÿæˆå®Œæ•´ã€è¿è´¯çš„å…¨å±€ç®€æŠ¥ã€‚",
            temperature=0.3,
            json_format=False
        )
        
        # ç¡®ä¿æ ¼å¼æ­£ç¡®
        if not final_summary.startswith("ğŸ“Š"):
            final_summary = f"ğŸ“Š {time_range_str}\n\n{final_summary}"
        
        logger.info(f"å…¨å±€æ‘˜è¦èšåˆå®Œæˆï¼Œé•¿åº¦ï¼š{len(final_summary)} å­—ç¬¦")
        return final_summary
    except Exception as e:
        logger.error(f"å…¨å±€æ‘˜è¦èšåˆå¤±è´¥: {e}")
        # å¦‚æœèšåˆå¤±è´¥ï¼Œå›é€€åˆ°æ‹¼æ¥æ‰€æœ‰åˆ†å—æ‘˜è¦
        fallback_summary = f"ğŸ“Š {time_range_str}\n\n"
        for i, chunk_result in enumerate(chunk_summaries):
            fallback_summary += f"\n=== åˆ†å— {i+1} ===\n{chunk_result['summary']}\n"
        return fallback_summary

def get_last_launch_time():
    """ä»ç®€æŠ¥æ–‡ä»¶åä¸­è·å–ä¸Šæ¬¡å¯åŠ¨æ—¶é—´"""
    vault_path = config.obsidian_vault_path
    if not vault_path or not os.path.exists(vault_path):
        return None
    
    # æŸ¥æ‰¾æ‰€æœ‰ç®€æŠ¥æ–‡ä»¶
    pattern = re.compile(r'ç®€æŠ¥_(\d{10})_-(\d{10})(?:_\d+)?\.md')
    last_time = None
    
    for filename in os.listdir(vault_path):
        match = pattern.match(filename)
        if match:
            end_time_str = match.group(2)  # æ–‡ä»¶åä¸­çš„ç»“æŸæ—¶é—´
            try:
                # è§£ææ—¶é—´ï¼šYYMMDDHHMM
                end_time = datetime.strptime(end_time_str, "%y%m%d%H%M")
                if last_time is None or end_time > last_time:
                    last_time = end_time
            except ValueError:
                continue
    
    return last_time

def generate_filename(start_time, end_time, index=None):
    """ç”Ÿæˆç®€æŠ¥æ–‡ä»¶å"""
    start_str = start_time.strftime("%y%m%d%H%M")
    end_str = end_time.strftime("%y%m%d%H%M")
    
    if index is None:
        return f"ç®€æŠ¥_{start_str}_-{end_str}.md"
    else:
        return f"ç®€æŠ¥_{start_str}_-{end_str}_{index}.md"

def is_basic_operation_question(content):
    """åˆ¤æ–­æ˜¯å¦ä¸ºåŸºç¡€æ“ä½œé—®é¢˜"""
    basic_keywords = [
        # äº¤æ˜“æ‰€ç›¸å…³
        'ä¸‹è½½äº¤æ˜“æ‰€', 'äº¤æ˜“æ‰€app', 'äº¤æ˜“æ‰€ä¸‹è½½', 'äº¤æ˜“æ‰€å®‰è£…',
        'å¸å®‰ä¸‹è½½', 'okxä¸‹è½½', 'ç«å¸ä¸‹è½½', 'gateä¸‹è½½',
        'å¸å®‰app', 'okx app', 'ç«å¸app',
        # Telegramç›¸å…³
        'telegramä¸­æ–‡', 'tgä¸­æ–‡', 'telegramè®¾ç½®ä¸­æ–‡', 'tgè®¾ç½®ä¸­æ–‡',
        'telegramè¯­è¨€', 'tgè¯­è¨€', 'telegramæ€ä¹ˆ', 'tgæ€ä¹ˆ',
        # Uniswapç›¸å…³
        'ä¸‹è½½uniswap', 'uniswap app', 'uniswapä¸‹è½½', 'uniswapå®‰è£…',
        'uniswapæ€ä¹ˆ',
        # é€šç”¨æ“ä½œ
        'æ€ä¹ˆä¸‹è½½', 'å¦‚ä½•ä¸‹è½½', 'æ€ä¹ˆå®‰è£…', 'å¦‚ä½•å®‰è£…',
        'æ€ä¹ˆç”¨', 'å¦‚ä½•ä½¿ç”¨', 'æ€ä¹ˆæ“ä½œ', 'å¦‚ä½•æ“ä½œ',
        'æ–°æ‰‹æ•™ç¨‹', 'å…¥é—¨æ•™ç¨‹', 'åŸºç¡€æ•™ç¨‹', 'æ•™ç¨‹',
        # é’±åŒ…ç›¸å…³
        'ä¸‹è½½é’±åŒ…', 'é’±åŒ…app', 'é’±åŒ…ä¸‹è½½', 'é’±åŒ…å®‰è£…',
        'metamaskä¸‹è½½', 'å°ç‹ç‹¸ä¸‹è½½', 'tpé’±åŒ…ä¸‹è½½',
        'é’±åŒ…æ€ä¹ˆ',
    ]
    
    content_lower = content.lower()
    for keyword in basic_keywords:
        if keyword in content_lower:
            return True
    return False

def count_basic_operation_questions(messages):
    """ç»Ÿè®¡åŸºç¡€æ“ä½œé—®é¢˜çš„æ•°é‡"""
    count = 0
    for msg in messages:
        if is_basic_operation_question(msg.content):
            count += 1
    return count

def filter_basic_operation_questions(messages):
    """è¿‡æ»¤æ‰åŸºç¡€æ“ä½œé—®é¢˜"""
    filtered_messages = []
    for msg in messages:
        if not is_basic_operation_question(msg.content):
            filtered_messages.append(msg)
    return filtered_messages

def save_report_stats(start_time, end_time, basic_op_count, filename):
    """ä¿å­˜ç®€æŠ¥ç»Ÿè®¡æ•°æ®"""
    stats_dir = "data/report_stats"
    if not os.path.exists(stats_dir):
        os.makedirs(stats_dir)
    
    stats_file = os.path.join(stats_dir, "report_stats.json")
    
    # è®¡ç®—ç»Ÿè®¡å°æ—¶æ•°
    hours = (end_time - start_time).total_seconds() / 3600
    
    # åˆ›å»ºç»Ÿè®¡è®°å½•
    stats_record = {
        "filename": filename,
        "start_time": start_time.isoformat(),
        "end_time": end_time.isoformat(),
        "hours": round(hours, 2),
        "basic_operation_count": basic_op_count,
        "basic_operation_density": round(basic_op_count / hours, 4) if hours > 0 else 0,
        "created_at": datetime.now().isoformat()
    }
    
    # è¯»å–ç°æœ‰ç»Ÿè®¡æ•°æ®
    all_stats = []
    if os.path.exists(stats_file):
        try:
            with open(stats_file, 'r', encoding='utf-8') as f:
                all_stats = json.load(f)
        except:
            all_stats = []
    
    # æ·»åŠ æ–°è®°å½•
    all_stats.append(stats_record)
    
    # åªä¿ç•™æœ€è¿‘100æ¡è®°å½•
    if len(all_stats) > 100:
        all_stats = all_stats[-100:]
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(all_stats, f, ensure_ascii=False, indent=2)
    
    return stats_record

def save_training_data(messages, basic_question_ids):
    """ä¿å­˜è®­ç»ƒæ•°æ®åˆ°CSVæ–‡ä»¶ï¼Œç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ"""
    import csv
    from datetime import datetime
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®ç›®å½•
    training_dir = "data/training_data"
    os.makedirs(training_dir, exist_ok=True)
    
    # è®­ç»ƒæ•°æ®æ–‡ä»¶
    training_file = os.path.join(training_dir, "basic_questions_training.csv")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶å†™å…¥è¡¨å¤´
    file_exists = os.path.exists(training_file)
    
    with open(training_file, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå†™å…¥è¡¨å¤´
        if not file_exists:
            writer.writerow(['message_id', 'message_text', 'is_basic_question', 'timestamp'])
        
        # ä¿å­˜æ‰€æœ‰æ¶ˆæ¯çš„è®­ç»ƒæ•°æ®
        for idx, msg in enumerate(messages):
            is_basic = 1 if idx in basic_question_ids else 0
            writer.writerow([
                f"{datetime.now().strftime('%Y%m%d')}_{idx}",
                msg.content[:500],  # é™åˆ¶é•¿åº¦ï¼Œé¿å…CSVé—®é¢˜
                is_basic,
                datetime.now().isoformat()
            ])
    
    logger.info(f"å·²ä¿å­˜ {len(messages)} æ¡è®­ç»ƒæ•°æ®åˆ° {training_file}")

def get_previous_report_stats():
    """è·å–ä¸Šæ¬¡ç®€æŠ¥çš„ç»Ÿè®¡æ•°æ®"""
    stats_file = "data/report_stats/report_stats.json"
    if not os.path.exists(stats_file):
        return None
    
    try:
        with open(stats_file, 'r', encoding='utf-8') as f:
            all_stats = json.load(f)
        
        if not all_stats:
            return None
        
        # è¿”å›æœ€è¿‘çš„ä¸€æ¡è®°å½•ï¼ˆæ’é™¤å½“å‰æ­£åœ¨å¤„ç†çš„ï¼‰
        return all_stats[-1]
    except:
        return None

def calculate_basic_op_density_change(current_stats, previous_stats):
    """è®¡ç®—åŸºç¡€æ“ä½œé—®é¢˜å¯†åº¦å˜åŒ–"""
    if previous_stats is None:
        # ç¬¬ä¸€æ¬¡ç®€æŠ¥ï¼Œä¸Šæ¬¡å¯†åº¦ä¸º0
        previous_density = 0
    else:
        previous_density = previous_stats.get("basic_operation_density", 0)
    
    current_density = current_stats.get("basic_operation_density", 0)
    
    # è®¡ç®—å˜åŒ–ï¼šå½“å‰å¯†åº¦ - ä¸Šæ¬¡å¯†åº¦
    density_change = current_density - previous_density
    
    # è¿”å›æ ¼å¼åŒ–åçš„ç»“æœï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰
    return f"{density_change:+.2f}" if density_change >= 0 else f"{density_change:.2f}"

def save_to_obsidian(content, filename):
    vault_path = config.obsidian_vault_path
    if not vault_path:
        logger.warning("æœªé…ç½® OBSIDIAN_VAULT_PATHï¼Œè·³è¿‡ä¿å­˜")
        return
    if not os.path.exists(vault_path):
        os.makedirs(vault_path)
    
    file_path = os.path.join(vault_path, filename)
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(content)
    logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ° Obsidian: {file_path}")

async def main():
    logger.info("å¼€å§‹ç”Ÿæˆæ·±åº¦ç®€æŠ¥...")
    
    # è°ƒè¯•ï¼šæ£€æŸ¥é…ç½®æ˜¯å¦æ­£ç¡®åŠ è½½
    for acc in config.collector_accounts:
        logger.info(f"è´¦å· {acc.account_id} ç›‘æ§ç¾¤ç»„æ•°é‡: {len(acc.monitored_chats) if acc.monitored_chats else 0}")
    
    # è·å–å½“å‰æ—¶é—´
    current_time = datetime.now()
    logger.info(f"å½“å‰æ—¶é—´: {current_time}")
    
    # è·å–ä¸Šæ¬¡å¯åŠ¨æ—¶é—´
    last_launch_time = get_last_launch_time()
    
    # ç¡®å®šæ—¶é—´çª—å£
    if last_launch_time is None:
        # ç¬¬ä¸€æ¬¡å¯åŠ¨ï¼šç»Ÿè®¡å‰24å°æ—¶
        logger.info("ç¬¬ä¸€æ¬¡å¯åŠ¨ï¼Œç»Ÿè®¡å‰24å°æ—¶å†…å®¹")
        end_time = current_time
        start_time = current_time - timedelta(hours=24)
        time_windows = [(start_time, end_time)]
    else:
        # è®¡ç®—æ—¶é—´é—´éš”ï¼ˆå°æ—¶ï¼‰
        time_diff = (current_time - last_launch_time).total_seconds() / 3600
        logger.info(f"è·ç¦»ä¸Šæ¬¡å¯åŠ¨æ—¶é—´: {time_diff:.2f} å°æ—¶")
        
        if time_diff <= 48:
            # æƒ…å†µ1ï¼šé—´éš” â‰¤ 48å°æ—¶
            logger.info("æ—¶é—´é—´éš” â‰¤ 48å°æ—¶ï¼Œç”Ÿæˆ1ä»½ç®€æŠ¥")
            start_time = last_launch_time
            end_time = current_time
            time_windows = [(start_time, end_time)]
        elif time_diff <= 120:
            # æƒ…å†µ2ï¼šé—´éš” > 48å°æ—¶ ä¸” â‰¤ 120å°æ—¶
            logger.info("æ—¶é—´é—´éš” 48-120å°æ—¶ï¼Œç”Ÿæˆ2ä»½ç®€æŠ¥")
            split_time = current_time - timedelta(hours=48)
            time_windows = [
                (last_launch_time, split_time),  # ç¬¬ä¸€ä»½ï¼šä¸Šæ¬¡å¯åŠ¨æ—¶é—´ â†’ 48å°æ—¶å‰
                (split_time, current_time)       # ç¬¬äºŒä»½ï¼š48å°æ—¶å‰ â†’ å½“å‰æ—¶é—´
            ]
        else:
            # æƒ…å†µ3ï¼šé—´éš” > 120å°æ—¶
            logger.info("æ—¶é—´é—´éš” > 120å°æ—¶ï¼Œç”Ÿæˆ1ä»½ç®€æŠ¥ï¼ˆæœ€è¿‘48å°æ—¶ï¼‰")
            start_time = current_time - timedelta(hours=48)
            end_time = current_time
            time_windows = [(start_time, end_time)]
    
    # è®°å½•æ—¶é—´çª—å£
    for i, (start, end) in enumerate(time_windows):
        logger.info(f"æ—¶é—´çª—å£ {i+1}: {start} è‡³ {end}")
    
    # åˆå§‹åŒ– AI æ€»ç»“å™¨
    summarizer = AISummarizer(
        api_key=config.ai_config.deepseek_api_key, 
        base_url=config.ai_config.openai_base_url
    )
    
    async with TelegramMultiAccountAdapter() as adapter:
        # å¤„ç†æ¯ä¸ªæ—¶é—´çª—å£
        for i, (start_time, end_time) in enumerate(time_windows):
            logger.info(f"æ­£åœ¨å¤„ç†æ—¶é—´çª—å£ {i+1}/{len(time_windows)}: {start_time} è‡³ {end_time}")
            
            logger.info("æ­£åœ¨å¹¶å‘é‡‡é›†æ¶ˆæ¯...")
            
            # limit_per_chat æ ¹æ®æ—¶é—´é—´éš”è°ƒæ•´
            hours_diff = (end_time - start_time).total_seconds() / 3600
            limit_per_chat = min(300, int(hours_diff * 12.5))  # å¤§çº¦æ¯å°æ—¶12.5æ¡
            
            unified_messages = await adapter.fetch_messages_concurrently(
                start_time=start_time,
                end_time=end_time,
                limit_per_chat=limit_per_chat
            )
            
            if not unified_messages:
                logger.info(f"æ—¶é—´çª—å£ {i+1} å†…æ²¡æœ‰æŠ“å–åˆ°æ–°æ¶ˆæ¯")
                continue

            # ç»Ÿè®¡åŸºç¡€æ“ä½œé—®é¢˜æ•°é‡
            basic_op_count = count_basic_operation_questions(unified_messages)
            logger.info(f"æ£€æµ‹åˆ°åŸºç¡€æ“ä½œé—®é¢˜æ•°é‡: {basic_op_count}")
            
            # è¿‡æ»¤æ‰åŸºç¡€æ“ä½œé—®é¢˜
            filtered_messages = filter_basic_operation_questions(unified_messages)
            logger.info(f"è¿‡æ»¤åå‰©ä½™æ¶ˆæ¯æ•°é‡: {len(filtered_messages)}")

            chat_contents = {}
            for msg in filtered_messages:
                chat_name = msg.chat_name or msg.chat_id
                if chat_name not in chat_contents:
                    chat_contents[chat_name] = []
                chat_contents[chat_name].append(msg.content)
                
            aggregated_input = ""
            total_messages_count = 0

            for chat_name, contents in chat_contents.items():
                # ä¸å†é™åˆ¶æ¯ä¸ªç¾¤ç»„çš„æ¶ˆæ¯æ•°é‡ï¼Œä½¿ç”¨æ‰€æœ‰æ¶ˆæ¯
                chat_slice = contents  # ä½¿ç”¨æ‰€æœ‰æ¶ˆæ¯
                aggregated_input += f"### Group: {chat_name}\n" + "\n".join(chat_slice) + "\n\n"
                total_messages_count += len(chat_slice)
            
            logger.info(f"æˆåŠŸæŠ“å– {len(unified_messages)} æ¡å»é‡åçš„æ¶ˆæ¯ï¼Œè¿‡æ»¤åå‰©ä½™ {len(filtered_messages)} æ¡ï¼Œå°†å¤„ç†æ‰€æœ‰ {total_messages_count} æ¡æ¶ˆæ¯")
            
            logger.info("æ­£åœ¨è°ƒç”¨ AI ç”Ÿæˆæ·±åº¦ç®€æŠ¥...")
            # ä¼ é€’å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ç»™AIï¼Œè®©AIè¯†åˆ«åŸºç¡€æ“ä½œé—®é¢˜
            summary_result = await generate_global_summary(summarizer, aggregated_input, unified_messages, start_time, end_time)
            
            # ä»JSONç»“æœä¸­æå–ç®€æŠ¥å†…å®¹å’ŒåŸºç¡€é—®é¢˜IDåˆ—è¡¨
            report_content = summary_result.get('summary', '')
            basic_question_ids = summary_result.get('basic_question_ids', [])
            
            # è®¡ç®—åŸºç¡€é—®é¢˜å¯†åº¦ï¼ˆç¨‹åºè®¡ç®—ï¼Œç¡®ä¿å‡†ç¡®ï¼‰
            total_messages = len(unified_messages)
            basic_op_count = len(basic_question_ids)
            basic_op_density = basic_op_count / total_messages if total_messages > 0 else 0
            
            logger.info(f"AIè¯†åˆ«åŸºç¡€æ“ä½œé—®é¢˜æ•°é‡: {basic_op_count} (å¯†åº¦: {basic_op_density:.2%})")
            
            # ä¿å­˜è®­ç»ƒæ•°æ®
            save_training_data(unified_messages, basic_question_ids)
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆå¤„ç†åŒä¸€å¤©å¤šæ¬¡å¯åŠ¨çš„æƒ…å†µï¼‰
            filename = generate_filename(start_time, end_time, i+1 if len(time_windows) > 1 else None)
            
            # è·å–ä¸Šæ¬¡ç®€æŠ¥ç»Ÿè®¡æ•°æ®
            previous_stats = get_previous_report_stats()
            
            # ä¿å­˜æœ¬æ¬¡ç®€æŠ¥ç»Ÿè®¡æ•°æ®
            current_stats = save_report_stats(start_time, end_time, basic_op_count, filename)
            
            # è®¡ç®—åŸºç¡€æ“ä½œé—®é¢˜å¯†åº¦å˜åŒ–
            density_change = calculate_basic_op_density_change(current_stats, previous_stats)
            
            # åœ¨æŠ¥å‘Šå†…å®¹åæ·»åŠ åŸºç¡€æ“ä½œé—®é¢˜å¯†åº¦ç»Ÿè®¡
            density_stats = f"""
## ğŸ“Š åŸºç¡€æ“ä½œé—®é¢˜ç»Ÿè®¡
- æ€»æ¶ˆæ¯æ•°: {total_messages}
- åŸºç¡€æ“ä½œé—®é¢˜æ•°: {basic_op_count}
- åŸºç¡€é—®é¢˜å¯†åº¦: {basic_op_density:.2%}
- å¯†åº¦å˜åŒ–: {density_change}
"""
            enhanced_report_content = f"{report_content}\n\n{density_stats}"
            
            # ä¿å­˜åˆ° Obsidian
            save_to_obsidian(enhanced_report_content, filename)
            
            # æ¨é€åˆ° Telegram
            await adapter.send_digest_to_channel(enhanced_report_content)
            logger.info(f"ç®€æŠ¥ {i+1} å·²æ¨é€åˆ° Telegram é¢‘é“")

if __name__ == "__main__":
    asyncio.run(main())
